{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaadb466-5bde-4980-b898-fe80cb83ce14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Relational Synthetic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2683ec19-1870-48ff-ad10-1af6e3b56b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas numpy faker scipy plotly dash databricks-langchain presidio-analyzer presidio-anonymizer\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da2b6f1f-b95b-43ec-9875-bc81de056756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Databricks Configuration\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import re\n",
    "from faker import Faker\n",
    "import hashlib\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Databricks configuration (reviewed and completed: added max_tokens=2000 for code generation length, temperature=0.1 for consistency)\n",
    "os.environ[\"DATABRICKS_HOST\"] = \"https://dbc-574f3c72-d3c1.cloud.databricks.com\"\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = \"dapi269b919a70b68566ee588f0075eb246a\"\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    max_tokens=6000,  # Reasonable limit for generating Python code\n",
    "    temperature=0.1   # Low for factual, consistent code generation\n",
    ")\n",
    "\n",
    "print(\"Configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d217b02-c2e0-42d9-b28e-74c500176f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Data Ingestion\n",
    "# Function to ingest CSV files from a folder path\n",
    "def ingest_data(folder_path: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Ingest all CSV files from the given folder path into a dictionary of DataFrames.\n",
    "    Assumes files are named appropriately (e.g., fact_table.csv, dim_customer.csv).\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    dataframes = {}\n",
    "    for file_path in folder.glob(\"*.csv\"):\n",
    "        table_name = file_path.stem  # Use filename without extension as table name\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes[table_name] = df\n",
    "        print(f\"Loaded {table_name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    return dataframes\n",
    "\n",
    "# Example usage: Replace with your actual folder path\n",
    "folder_path = \"/Workspace/Users/geoj5official@gmail.com/02_Relational Data/New_Syn\"  # Update this path\n",
    "real_data = ingest_data(folder_path)\n",
    "\n",
    "if not real_data:\n",
    "    raise ValueError(\"No CSV files found in the specified folder. Please check the path.\")\n",
    "\n",
    "print(f\"Ingested {len(real_data)} tables: {list(real_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f941bdc3-5492-4241-8463-66f2e266e92d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Metadata Extraction, Schema Inference, Fact Table Identification, and Relationships\n",
    "def extract_metadata(data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract metadata including schema, infer Star Schema (fact and dimension tables),\n",
    "    detect relationships (FKs), and store in JSON-compatible dict.\n",
    "    - Fact table: Identified as the table with the most numerical (measure) columns or largest size.\n",
    "    - Relationships: Inferred by matching column names (e.g., customer_id in fact to id in dim).\n",
    "    - If no clear Star Schema, infer meaningful joins based on common keys.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"tables\": {},\n",
    "        \"relationships\": [],\n",
    "        \"fact_table\": None,\n",
    "        \"dimension_tables\": []\n",
    "    }\n",
    "    \n",
    "    # Analyze each table\n",
    "    for table_name, df in data.items():\n",
    "        dtypes = df.dtypes.to_dict()\n",
    "        num_cols = [col for col, dtype in dtypes.items() if np.issubdtype(dtype, np.number)]\n",
    "        cat_cols = [col for col, dtype in dtypes.items() if dtype == 'object']\n",
    "        \n",
    "        metadata[\"tables\"][table_name] = {\n",
    "            \"columns\": [{\"name\": col, \"dtype\": str(dtype)} for col, dtype in dtypes.items()],\n",
    "            \"shape\": list(df.shape),\n",
    "            \"numerical_columns\": num_cols,\n",
    "            \"categorical_columns\": cat_cols,\n",
    "            \"sample_size\": min(100, len(df))  # For later sampling\n",
    "        }\n",
    "        \n",
    "        # Score for fact table: higher if more numerical columns and larger size\n",
    "        fact_score = len(num_cols) * len(df)\n",
    "        metadata[\"tables\"][table_name][\"fact_score\"] = fact_score\n",
    "    \n",
    "    # Identify fact table: highest fact_score\n",
    "    if data:\n",
    "        fact_table = max(metadata[\"tables\"].keys(), key=lambda k: metadata[\"tables\"][k][\"fact_score\"])\n",
    "        metadata[\"fact_table\"] = fact_table\n",
    "        metadata[\"dimension_tables\"] = [t for t in metadata[\"tables\"] if t != fact_table]\n",
    "    \n",
    "    # Infer relationships: Look for potential FKs (e.g., *_id in fact matching id in dim)\n",
    "    fact_df = data[metadata[\"fact_table\"]] if metadata[\"fact_table\"] else list(data.values())[0]\n",
    "    fact_cols = set(fact_df.columns)\n",
    "    \n",
    "    for dim_name in metadata[\"dimension_tables\"]:\n",
    "        dim_df = data[dim_name]\n",
    "        dim_cols = set(dim_df.columns)\n",
    "        potential_pk = next((col for col in dim_cols if col in ['id', 'ID'] or col.endswith('_id')), None)\n",
    "        \n",
    "        if potential_pk:\n",
    "            # Look for FK in fact: e.g., dim_name + '_id'\n",
    "            potential_fk = next((col for col in fact_cols if col.startswith(dim_name.replace('dim_', '')) and col.endswith('_id')), None)\n",
    "            if potential_fk and potential_pk in dim_df.columns:\n",
    "                # Quick check: unique in dim, many in fact\n",
    "                if dim_df[potential_pk].nunique() < len(dim_df) * 0.9 and fact_df[potential_fk].nunique() > 10:\n",
    "                    metadata[\"relationships\"].append({\n",
    "                        \"from_table\": fact_table,\n",
    "                        \"from_column\": potential_fk,\n",
    "                        \"to_table\": dim_name,\n",
    "                        \"to_column\": potential_pk,\n",
    "                        \"type\": \"one-to-many\"  # Assuming dim to fact\n",
    "                    })\n",
    "                    print(f\"Inferred relationship: {fact_table}.{potential_fk} -> {dim_name}.{potential_pk}\")\n",
    "    \n",
    "    # If no relationships inferred, add basic ones based on common columns\n",
    "    all_cols = {t: set(df.columns) for t, df in data.items()}\n",
    "    for t1 in all_cols:\n",
    "        for t2 in all_cols:\n",
    "            if t1 != t2:\n",
    "                common = all_cols[t1] & all_cols[t2]\n",
    "                if common:\n",
    "                    metadata[\"relationships\"].append({\n",
    "                        \"from_table\": t1,\n",
    "                        \"from_column\": list(common)[0],  # Take first common\n",
    "                        \"to_table\": t2,\n",
    "                        \"to_column\": list(common)[0],\n",
    "                        \"type\": \"inferred\"\n",
    "                    })\n",
    "                    print(f\"Inferred common column relationship: {t1} <-> {t2} on {list(common)[0]}\")\n",
    "                    break  # Limit to one per pair\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "metadata = extract_metadata(real_data)\n",
    "\n",
    "# Store metadata in JSON\n",
    "with open(\"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(\"Metadata extracted and saved to metadata.json\")\n",
    "print(json.dumps({k: v for k, v in metadata.items() if k != 'tables'}, indent=2))  # Print summary excluding full tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3115ea4f-fb21-4999-91c0-40e465ac0244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Cell 4: Sample Data with Overlap and PII Masking\n",
    "# def sample_and_mask(data: Dict[str, pd.DataFrame], metadata: Dict[str, Any], sample_frac: float = 0.2) -> Dict[str, pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     Take overlapping samples (same rows across related tables via keys) and mask PII.\n",
    "#     - Overlap: Sample based on FK joins.\n",
    "#     - PII: Detect names, emails, SSNs via regex and hash.\n",
    "#     \"\"\"\n",
    "#     samples = {}\n",
    "#     fake = Faker()\n",
    "    \n",
    "#     # PII patterns\n",
    "#     pii_patterns = {\n",
    "#         'name': r'^[A-Z][a-z]+ [A-Z][a-z]+$',  # Simple name\n",
    "#         'email': r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$',\n",
    "#         'ssn': r'^\\d{3}-\\d{2}-\\d{4}$'\n",
    "#     }\n",
    "    \n",
    "#     for table_name, df in data.items():\n",
    "#         # Sample\n",
    "#         if len(df) > 1000:\n",
    "#             # For overlap: If related to fact, sample based on fact sample\n",
    "#             if metadata[\"fact_table\"] and table_name != metadata[\"fact_table\"]:\n",
    "#                 fact_df = data[metadata[\"fact_table\"]]\n",
    "#                 rel = next((r for r in metadata[\"relationships\"] if r[\"from_table\"] == metadata[\"fact_table\"] and r[\"to_table\"] == table_name), None)\n",
    "#                 if rel:\n",
    "#                     fk_col = rel[\"from_column\"]\n",
    "#                     sample_keys = fact_df[fk_col].dropna().sample(frac=sample_frac, random_state=42).unique()\n",
    "#                     df_sample = df[df[rel[\"to_column\"]].isin(sample_keys)]\n",
    "#                 else:\n",
    "#                     df_sample = df.sample(frac=sample_frac, random_state=42)\n",
    "#             else:\n",
    "#                 df_sample = df.sample(frac=sample_frac, random_state=42)\n",
    "#         else:\n",
    "#             df_sample = df.copy()\n",
    "        \n",
    "#         samples[table_name] = df_sample.reset_index(drop=True)\n",
    "        \n",
    "#         # Mask PII\n",
    "#         for col in df_sample.columns:\n",
    "#             if df_sample[col].dtype == 'object':\n",
    "#                 for row_idx, val in enumerate(df_sample[col]):\n",
    "#                     if pd.isna(val):\n",
    "#                         continue\n",
    "#                     val_str = str(val).strip()\n",
    "#                     for pii_type, pattern in pii_patterns.items():\n",
    "#                         if re.match(pattern, val_str):\n",
    "#                             # Hash for anonymization\n",
    "#                             hashed = hashlib.sha256(val_str.encode()).hexdigest()[:10]\n",
    "#                             samples[table_name].at[row_idx, col] = f\"{pii_type}_masked_{hashed}\"\n",
    "#                             break\n",
    "        \n",
    "#         print(f\"Sampled and masked {table_name}: {len(samples[table_name])} rows\")\n",
    "    \n",
    "#     return samples\n",
    "\n",
    "# sample_data = sample_and_mask(real_data, metadata)\n",
    "\n",
    "# # Update metadata with sample info\n",
    "# metadata[\"samples\"] = {t: list(df.shape) for t, df in sample_data.items()}\n",
    "\n",
    "# with open(\"metadata.json\", \"w\") as f:\n",
    "#     json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "# print(\"Samples created and PII masked. Updated metadata.json\")\n",
    "\n",
    "\n",
    "\n",
    "# Cell 4: Sample Data with Overlap (No PII Handling)\n",
    "def sample_with_overlap(data: Dict[str, pd.DataFrame], metadata: Dict[str, Any], sample_frac: float = 0.2) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Take overlapping samples (same rows across related tables via keys).\n",
    "    - Overlap: Ensure minimal overlap across related tables for referential integrity.\n",
    "    - No PII handling.\n",
    "    \"\"\"\n",
    "    samples = {}\n",
    "\n",
    "    for table_name, df in data.items():\n",
    "        if len(df) > 1000:\n",
    "            # If table is related to the fact table, ensure overlap via FK joins\n",
    "            if metadata.get(\"fact_table\") and table_name != metadata[\"fact_table\"]:\n",
    "                fact_df = data[metadata[\"fact_table\"]]\n",
    "                rel = next(\n",
    "                    (r for r in metadata[\"relationships\"]\n",
    "                     if r[\"from_table\"] == metadata[\"fact_table\"]\n",
    "                     and r[\"to_table\"] == table_name),\n",
    "                    None\n",
    "                )\n",
    "                if rel:\n",
    "                    fk_col = rel[\"from_column\"]\n",
    "                    to_col = rel[\"to_column\"]\n",
    "\n",
    "                    # Get a minimal overlapping key set for relational consistency\n",
    "                    sample_keys = (\n",
    "                        fact_df[fk_col]\n",
    "                        .dropna()\n",
    "                        .sample(frac=sample_frac / 2, random_state=42)\n",
    "                        .unique()\n",
    "                    )\n",
    "\n",
    "                    df_sample = df[df[to_col].isin(sample_keys)]\n",
    "\n",
    "                    # Add a small random fraction to avoid full overlap\n",
    "                    extra_sample = df.sample(frac=sample_frac / 10, random_state=99)\n",
    "                    df_sample = pd.concat([df_sample, extra_sample]).drop_duplicates()\n",
    "\n",
    "                else:\n",
    "                    df_sample = df.sample(frac=sample_frac, random_state=42)\n",
    "            else:\n",
    "                df_sample = df.sample(frac=sample_frac, random_state=42)\n",
    "        else:\n",
    "            df_sample = df.copy()\n",
    "\n",
    "        samples[table_name] = df_sample.reset_index(drop=True)\n",
    "        print(f\"Sampled {table_name}: {len(samples[table_name])} rows\")\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Generate the samples\n",
    "sample_data = sample_with_overlap(real_data, metadata)\n",
    "\n",
    "# Update metadata with sample info\n",
    "metadata[\"samples\"] = {t: list(df.shape) for t, df in sample_data.items()}\n",
    "\n",
    "with open(\"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(\"Samples created with minimal overlap. Updated metadata.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01a68bd-91ca-4beb-b8d2-cb40a090e875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: LLM-based Code Generator \n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Any\n",
    "\n",
    "# User input for row counts per table\n",
    "row_counts = {}\n",
    "for table_name in metadata['tables']:\n",
    "    default_rows = 200 if table_name != metadata['fact_table'] else 1000\n",
    "    user_input = input(f\"Enter number of rows for table '{table_name}' (default {default_rows}): \") or str(default_rows)\n",
    "    row_counts[table_name] = int(user_input)\n",
    "\n",
    "def generate_synthetic_code(metadata: Dict[str, Any], sample_data: Dict[str, pd.DataFrame]) -> str:\n",
    "    # Compute strings for insertion\n",
    "    fact_table_str = metadata.get('fact_table', 'unknown')\n",
    "    dim_tables_str = ', '.join(metadata.get('dimension_tables', []))\n",
    "    metadata_json = json.dumps(metadata, indent=2)\n",
    "    row_counts_json = json.dumps(row_counts, indent=2)\n",
    "    \n",
    "    # Prepare a highly structured prompt with escaped braces for code placeholders\n",
    "    prompt = \"\"\"You are an expert Python data engineer. Generate a COMPLETE, self-contained, executable Python script to create synthetic relational data based on the given Star Schema metadata and samples. The script MUST run without errors in a standard Python environment with pandas, numpy, faker, and databricks-langchain for Llama model integration.\n",
    "\n",
    "MANDATORY CODE STRUCTURE:\n",
    "1. Imports at the VERY TOP (exact lines):\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import os\n",
    "from datetime import date\n",
    "os.environ[\"DATABRICKS_HOST\"] = \"https://dbc-574f3c72-d3c1.cloud.databricks.com\"\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = \"dapi269b919a70b68566ee588f0075eb246a\"\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "2. Set up Llama model IMMEDIATELY after imports (exact):\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    max_tokens=6000,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "3. Set seeds AFTER model setup (exact):\n",
    "np.random.seed(42)\n",
    "Faker.seed(42)  # Class method, NOT instance.seed()\n",
    "fake = Faker()  # Then instantiate\n",
    "os.makedirs('synthetic_data', exist_ok=True)  # Create output folder if needed\n",
    "\n",
    "4. Generate dimension tables FIRST (with specified row counts from row_counts dict):\n",
    "   - Use row_counts = {row_counts_json}  # Hardcode this dict in the code\n",
    "   - For each dimension table, create a DataFrame with row_counts[table_name] rows.\n",
    "   - Ensure STRICT UNIQUENESS: Dimension tables MUST have unique primary keys (PKs) and NO duplicate rows overall. They represent distinct entities. Use pd.drop_duplicates() after generation if needed, and regenerate if row count drops below required.\n",
    "   - For each column (based on metadata dtypes and samples):\n",
    "     - PRIORITIZE Faker for generation where possible (e.g., fake.name() for names, fake.date() for dates, fake.random_int() for ints, fake.random_number() for floats, fake.word() or fake.sentence() for text).\n",
    "     - If Faker is unsuitable (e.g., for domain-specific or patterned data like 'MASTER_KEY' that doesn't fit standard Faker providers, or when needing to match complex sample distributions/uniques not easily replicable with Faker), use the Llama model (chat_model) to generate values:\n",
    "       - Create a prompt like: \"Generate {{num_rows}} unique values similar to these samples: {{sample_values}}. Ensure they match dtype {{dtype}} and range {{min}}-{{max}}. Focus on realism, correlation, and uniqueness.\"\n",
    "       - Invoke chat_model.invoke(prompt).content, parse the response into a list, and assign to the column.\n",
    "       - Handle parsing carefully: Split by commas/newlines, convert to appropriate types, ensure no nulls and uniqueness.\n",
    "     - For numerical: If Faker fits (e.g., random_int), use it; else Llama. Match sample stats (mean/std/min/max) using np.random.normal or similar, but clip/adjust for negatives if present.\n",
    "     - For categorical/object: If uniques <20, np.random.choice; else Faker (e.g., fake.company() if business-like); if not fitting, use Llama to generate similar to samples. Ensure generated values are unique if part of PK.\n",
    "     - Add sequential PK (e.g., id = np.arange(1, row_counts[table_name]+1)), but if sample PK has negatives or different patterns, use Llama to generate matching unique keys.\n",
    "     - Ensure uniqueness for PKs: Use set or np.unique to enforce, and regenerate if duplicates occur.\n",
    "     - Avoid null/NaN: Fill any potential nulls with defaults (0 for nums, '' for strings, earliest date for dates).\n",
    "   - After generating all columns, ensure the entire DataFrame has no duplicate rows: dim_df = dim_df.drop_duplicates().reset_index(drop=True); while len(dim_df) < row_counts[table_name]: add more unique rows.\n",
    "   - Save each as pd.DataFrame(...).to_csv('synthetic_data/synthetic_{{table_name}}.csv', index=False)  # Use actual table_name in code\n",
    "\n",
    "5. Generate fact table LAST (with row_counts[fact_table] rows):\n",
    "   - Similar column generation as above: Prefer Faker, fallback to Llama for tricky columns.\n",
    "   - For each FK relationship: Set FK column to np.random.choice from actual dim PK values to ensure correlation and referential integrity (e.g., fk_values = dim_df['pk_col'].values; fact_df['fk_col'] = np.random.choice(fk_values, size=row_counts[fact_table], replace=True)). Allow repeats in FKs for many-to-one relationships. If dim PK has negatives, FK must match.\n",
    "   - Maintain overall correlations: If numerical columns in fact are correlated in samples (e.g., via pearsonr), use multivariate_normal or similar to preserve correlations. Strongly interweave logical correlations (e.g., higher quantity leads to higher total_price = quantity * unit_price).\n",
    "   - Even if real sample data lacks clear correlations, infer and enforce meaningful ones (e.g., if 'age' and 'income' in dim, make higher age correlate positively with income; for fact, ensure measures like sales correlate with dimensions like customer_type). Always prioritize correlated, realistic data as per user request.\n",
    "   - For any column where Faker doesn't suffice, use Llama with prompts to generate correlated values (e.g., \"Generate values for 'column' correlated with 'related_column' values: {{related_values}}\").\n",
    "   - Avoid null/NaN: Fill any potential nulls with defaults (0 for nums, '' for strings, earliest date for dates).\n",
    "   - Allow duplicate rows in fact table as it represents transactions/events, but ensure FK references are valid.\n",
    "\n",
    "6. Add inline comments for every major step (e.g., # Generate numerical column 'sales' using Faker if possible, else Llama, matching sample distribution, preserving floats/ints, enforcing correlations. # Ensure uniqueness in dim tables).\n",
    "\n",
    "Schema Details:\n",
    "Fact table: {fact_table}\n",
    "Dimension tables: {dim_tables}\n",
    "\n",
    "Full Metadata (use to guide dtypes and relationships):\n",
    "{metadata_json}\n",
    "\n",
    "Sample Data Snippets (use to compute means/stds/uniques; assume these are representative; infer correlations across columns/tables):\n",
    "\"\"\".format(row_counts_json=row_counts_json, fact_table=fact_table_str, dim_tables=dim_tables_str, metadata_json=metadata_json)\n",
    "\n",
    "    for table, df in sample_data.items():\n",
    "        prompt += f\"\\n\\n# Sample for '{table}' (shape: {df.shape}):\\n\"\n",
    "        for col in df.columns:\n",
    "            sample_col = df[col].dropna().head(5).tolist()\n",
    "            uniques = df[col].unique()[:10]  # Limit\n",
    "            if np.issubdtype(df[col].dtype, np.number):\n",
    "                mean = df[col].mean()\n",
    "                std = df[col].std()\n",
    "                min_val = df[col].min()\n",
    "                max_val = df[col].max()\n",
    "                mean_str = f\"{mean:.2f}\" if not pd.isna(mean) else \"nan\"\n",
    "                std_str = f\"{std:.2f}\" if not pd.isna(std) else \"nan\"\n",
    "                stats_str = f\" (mean: {mean_str}, std: {std_str}, min: {min_val}, max: {max_val}, uniques: {len(uniques)}, dtype: {df[col].dtype})\"\n",
    "            else:\n",
    "                stats_str = f\" (uniques: {len(uniques)}, dtype: {df[col].dtype})\"\n",
    "            prompt += f\"# Col '{col}'{stats_str}: sample values {sample_col}, uniques {list(uniques)}\\n\"\n",
    "\n",
    "    prompt += \"\"\"\n",
    "ADDITIONAL RULES:\n",
    "- Star Schema Compliance: Dimension tables MUST contain unique entities (unique PKs, no duplicate rows). Fact table links via FKs with possible repeats, modeling many-to-one relationships. Validate relationships from metadata.\n",
    "- Hybrid Approach: Always try Faker first for efficiency/realism (e.g., use specific providers like fake.uuid4() for keys if fitting). Only use Llama when Faker can't produce suitable data (e.g., custom patterns, domain-specific terms like 'MASTER_KEY' that need to mimic samples closely).\n",
    "- Llama Prompts: Make them specific, e.g., \"Generate {{n}} unique strings similar to: {{samples}}. Ensure variety and no duplicates.\" Parse output as list.\n",
    "- Handle negative values: If any column (esp. uniques/IDs) has negatives in samples, generate synthetic with similar range/distribution using appropriate methods.\n",
    "- Ensure correlations: Fact/dim IDs must be perfectly correlated via FK/PK matches (FKs repeat, PKs unique). Infer and preserve inter-column correlations (e.g., use np.corrcoef on samples, then generate correlated data). Strongly enforce logical correlations throughout (e.g., derived columns like total = price * qty). Use Llama if needed for correlated text generation.\n",
    "- Accurate types: Match exact dtypes from metadata/samples (e.g., float64 for floats, int64 for ints; use astype if needed).\n",
    "- Clear synthetic data: Even if input data is non-correlated/messy, generate clean, logically correlated data (e.g., realistic relationships between columns like price and quantity). Always make data more correlated as per user request.\n",
    "- No nulls: Ensure all generated data has no NaN/null; replace with sensible defaults based on column type and samples.\n",
    "- For dates: If any date-like columns, use fake.date_between(start_date=min_sample, end_date=max_sample), no nulls.\n",
    "- Relationships: Ensure FK values in fact exactly match existing dim PKs for integrity. No orphan FKs.\n",
    "- NO other imports (e.g., no random; use np.random).\n",
    "- NO print statements or extra output; just generate and save CSVs.\n",
    "- Make code auditable: Comments explain choice of Faker vs Llama, handling of negatives, correlations, dtypes, no nulls, uniqueness in dims, and star schema compliance.\n",
    "\n",
    "Output ONLY the Python code itself. No explanations, no markdown fences (e.g., no ```python). Start directly with 'import pandas...'.\n",
    "\"\"\"\n",
    "    \n",
    "    # Call LLM\n",
    "    response = chat_model.invoke(prompt)\n",
    "    generated_code = response.content.strip()\n",
    "    \n",
    "    # Clean any lingering markdown\n",
    "    generated_code = re.sub(r'^```python\\s*\\n?', '', generated_code, flags=re.MULTILINE)\n",
    "    generated_code = re.sub(r'\\n?```$', '', generated_code, flags=re.MULTILINE).strip()\n",
    "    \n",
    "    # Safety net: Prepend basics if missing\n",
    "    if not generated_code.startswith('import pandas'):\n",
    "        generated_code = \"\"\"import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import os\n",
    "os.environ[\"DATABRICKS_HOST\"] = \"https://dbc-574f3c72-d3c1.cloud.databricks.com\"\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = \"dapi269b919a70b68566ee588f0075eb246a\"\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    max_tokens=6000,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "Faker.seed(42)\n",
    "fake = Faker()\n",
    "os.makedirs('synthetic_data', exist_ok=True)\"\"\" + '\\n\\n' + generated_code\n",
    "    \n",
    "    # Save generated code\n",
    "    with open(\"generated_synthetic_code.py\", \"w\") as f:\n",
    "        f.write(generated_code)\n",
    "    \n",
    "    print(\"Updated generated code saved to generated_synthetic_code.py with per-table row counts, enhanced correlations, hybrid Faker + Llama approach, and strict star schema uniqueness.\")\n",
    "    return generated_code\n",
    "\n",
    "# Re-generate with updated prompt\n",
    "synth_code = generate_synthetic_code(metadata, sample_data)\n",
    "print(\"Generated Code Preview:\\n\", synth_code[:800], \"\\n...\")  # Longer preview for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33fb784-d6f8-4c43-9abd-771686092d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Execute Synthetic Code and Output CSVs (Fixed for TypeError)\n",
    "import re\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "def execute_synthetic_code(code_str: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Execute the generated code to produce synthetic DataFrames.\n",
    "    Fixed: Added dtype check for negative value comparisons to avoid TypeError for non-numeric columns.\n",
    "    Updated: Create 'synthetic_data' folder; load from there; check for no nulls post-execution.\n",
    "    \"\"\"\n",
    "    # Create output folder\n",
    "    os.makedirs(\"synthetic_data\", exist_ok=True)\n",
    "    \n",
    "    # Clean code\n",
    "    code_clean = re.sub(r'^```python\\s*\\n?', '', code_str, flags=re.MULTILINE)\n",
    "    code_clean = re.sub(r'\\n?```$', '', code_clean, flags=re.MULTILINE).strip()\n",
    "    \n",
    "    # Pre-pend corrected essential imports and seeding\n",
    "    pre_imports = \"\"\"import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import os\n",
    "# Correct seeding: Class method first\n",
    "np.random.seed(42)\n",
    "Faker.seed(42)\n",
    "fake = Faker()  # Now instantiate\n",
    "os.makedirs('synthetic_data', exist_ok=True)\n",
    "\"\"\"\n",
    "    \n",
    "    code_to_exec = pre_imports + \"\\n\\n\" + code_clean\n",
    "    \n",
    "    local_vars = {}\n",
    "    try:\n",
    "        exec(code_to_exec, {\"pd\": pd, \"np\": np, \"Faker\": Faker, \"os\": os}, local_vars)\n",
    "        print(\"Synthetic code executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing synthetic code: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"Code snippet causing issue:\\n\", code_clean[:300])\n",
    "        return {}\n",
    "    \n",
    "    synth_data = {}\n",
    "    for table_name in metadata[\"tables\"]:\n",
    "        csv_path = f\"synthetic_data/synthetic_{table_name}.csv\"\n",
    "        if Path(csv_path).exists():\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                synth_data[table_name] = df\n",
    "                print(f\"Loaded synthetic {table_name}: {df.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {csv_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: synthetic_{table_name}.csv not found in synthetic_data folder.\")\n",
    "    \n",
    "    # Post-execution checks\n",
    "    if synth_data:\n",
    "        print(\"Performing integrity checks...\")\n",
    "        # Check dtypes match real\n",
    "        for table_name, synth_df in synth_data.items():\n",
    "            real_df = real_data.get(table_name)\n",
    "            if real_df is not None:\n",
    "                for col in real_df.columns:\n",
    "                    if col in synth_df.columns:\n",
    "                        if real_df[col].dtype != synth_df[col].dtype:\n",
    "                            print(f\"Warning: dtype mismatch in {table_name}.{col}: real {real_df[col].dtype} vs synth {synth_df[col].dtype}\")\n",
    "        \n",
    "        # Check referential integrity\n",
    "        for rel in metadata[\"relationships\"]:\n",
    "            from_df = synth_data.get(rel[\"from_table\"])\n",
    "            to_df = synth_data.get(rel[\"to_table\"])\n",
    "            if from_df is not None and to_df is not None:\n",
    "                fk_vals = set(from_df[rel[\"from_column\"]].dropna().unique())\n",
    "                pk_vals = set(to_df[rel[\"to_column\"]].dropna().unique())\n",
    "                invalid_fks = fk_vals - pk_vals\n",
    "                if invalid_fks:\n",
    "                    print(f\"Warning: Invalid FKs in {rel['from_table']}.{rel['from_column']} -> {rel['to_table']}.{rel['to_column']}: {invalid_fks}\")\n",
    "                else:\n",
    "                    print(f\"Referential integrity OK for {rel['from_table']}.{rel['from_column']} -> {rel['to_table']}.{rel['to_column']}\")\n",
    "        \n",
    "        # Check for negatives if in real (only for numeric columns)\n",
    "        for table_name, real_df in real_data.items():\n",
    "            synth_df = synth_data.get(table_name)\n",
    "            if synth_df is not None:\n",
    "                for col in real_df.columns:\n",
    "                    if np.issubdtype(real_df[col].dtype, np.number):  # Check if column is numeric\n",
    "                        if (real_df[col] < 0).any():\n",
    "                            if col in synth_df.columns and not (synth_df[col] < 0).any():\n",
    "                                print(f\"Warning: No negatives in synth {table_name}.{col}, but present in real.\")\n",
    "        \n",
    "        # Check for no nulls\n",
    "        for table_name, synth_df in synth_data.items():\n",
    "            if synth_df.isnull().any().any():\n",
    "                print(f\"Warning: Null values found in synthetic {table_name}\")\n",
    "\n",
    "    return synth_data\n",
    "\n",
    "synth_data = execute_synthetic_code(synth_code)\n",
    "\n",
    "# Enhanced fallback if still fails (Updated for per-table rows, enhanced correlations, fixed TypeError)\n",
    "if not synth_data:\n",
    "    print(\"Enhanced fallback generation...\")\n",
    "    # Correct seeding\n",
    "    np.random.seed(42)\n",
    "    Faker.seed(42)\n",
    "    fake = Faker()\n",
    "    os.makedirs(\"synthetic_data\", exist_ok=True)\n",
    "    \n",
    "    # Order: dimensions first, then fact\n",
    "    dim_tables = [t for t in real_data.keys() if t != metadata[\"fact_table\"]]\n",
    "    all_tables = dim_tables + [metadata[\"fact_table\"]]\n",
    "    \n",
    "    synth_data = {}\n",
    "    dim_pks = {}  # Store actual PK values for dims\n",
    "    \n",
    "    for table_name in all_tables:\n",
    "        df_real = real_data[table_name]\n",
    "        n_rows = row_counts.get(table_name, 1000 if table_name == metadata[\"fact_table\"] else 200)  # Fallback default\n",
    "        \n",
    "        # Generate synthetic data with type matching, negatives, no nulls\n",
    "        synth_dict = {}\n",
    "        for col in df_real.columns:\n",
    "            col_data = df_real[col].dropna()\n",
    "            default_val = 0 if np.issubdtype(df_real[col].dtype, np.number) else '' if df_real[col].dtype == 'object' else pd.Timestamp('1900-01-01')\n",
    "            if len(col_data) == 0:\n",
    "                synth_dict[col] = np.full(n_rows, default_val)\n",
    "                continue\n",
    "            \n",
    "            dtype = df_real[col].dtype\n",
    "            if dtype == 'object':\n",
    "                # Categorical/text/date\n",
    "                try:\n",
    "                    is_date = all(isinstance(v, str) and re.match(r'\\d{4}-\\d{2}-\\d{2}', str(v)) for v in col_data.head(10) if v)\n",
    "                except TypeError:\n",
    "                    is_date = False\n",
    "                if is_date:\n",
    "                    synth_dict[col] = [fake.date_between(start_date='-1y', end_date='today') for _ in range(n_rows)]\n",
    "                else:\n",
    "                    uniques = col_data.unique()\n",
    "                    if len(uniques) < 10:\n",
    "                        synth_dict[col] = np.random.choice(uniques, n_rows)\n",
    "                    else:\n",
    "                        if 'name' in col.lower():\n",
    "                            synth_dict[col] = [fake.name() for _ in range(n_rows)]\n",
    "                        elif 'email' in col.lower():\n",
    "                            synth_dict[col] = [fake.email() for _ in range(n_rows)]\n",
    "                        elif 'address' in col.lower():\n",
    "                            synth_dict[col] = [fake.address() for _ in range(n_rows)]\n",
    "                        else:\n",
    "                            synth_dict[col] = [fake.word() for _ in range(n_rows)]\n",
    "            elif np.issubdtype(dtype, np.number):\n",
    "                # Numerical, handle negatives, match float/int\n",
    "                mean = col_data.mean()\n",
    "                std = col_data.std()\n",
    "                min_val = col_data.min()\n",
    "                max_val = col_data.max()\n",
    "                if pd.isna(std) or std == 0:\n",
    "                    synth_dict[col] = np.full(n_rows, mean if not pd.isna(mean) else 0)\n",
    "                else:\n",
    "                    synth_dict[col] = np.random.normal(mean, std, n_rows)\n",
    "                    # No clip if negatives present\n",
    "                    if min_val < 0:\n",
    "                        # Ensure some negatives\n",
    "                        synth_dict[col][synth_dict[col] > 0] -= np.random.uniform(0, abs(min_val), sum(synth_dict[col] > 0))\n",
    "                    else:\n",
    "                        synth_dict[col] = np.clip(synth_dict[col], min_val, max_val)\n",
    "                # Match dtype\n",
    "                if np.issubdtype(dtype, np.integer):\n",
    "                    synth_dict[col] = np.round(synth_dict[col]).astype(dtype)\n",
    "                else:\n",
    "                    synth_dict[col] = synth_dict[col].astype(dtype)\n",
    "            elif 'datetime' in str(dtype):\n",
    "                # Dates\n",
    "                min_date = col_data.min() if not col_data.empty else pd.Timestamp('1900-01-01')\n",
    "                max_date = col_data.max() if not col_data.empty else pd.Timestamp.now()\n",
    "                synth_dict[col] = [fake.date_time_between(start_date=min_date, end_date=max_date) for _ in range(n_rows)]\n",
    "            else:\n",
    "                # Fallback\n",
    "                synth_dict[col] = np.random.choice(col_data, n_rows)\n",
    "        \n",
    "        synth_df = pd.DataFrame(synth_dict)\n",
    "        \n",
    "        # Fill any remaining nulls (shouldn't happen)\n",
    "        synth_df = synth_df.fillna(default_val)\n",
    "        \n",
    "        # Add PK for dimension tables, match sample range if negatives\n",
    "        if table_name != metadata[\"fact_table\"]:\n",
    "            pk_col = next((col for col in synth_df.columns if col.lower() in ['id'] or col.lower().endswith('_id')), None)\n",
    "            if pk_col is None:\n",
    "                pk_col = 'id'\n",
    "                synth_df[pk_col] = range(1, n_rows + 1)\n",
    "            else:\n",
    "                real_pk = df_real[pk_col].dropna()\n",
    "                if np.issubdtype(real_pk.dtype, np.number) and (real_pk < 0).any():\n",
    "                    synth_df[pk_col] = np.random.choice(real_pk, n_rows, replace=True) if len(real_pk) > 0 else range(-n_rows, 0)\n",
    "                else:\n",
    "                    synth_df[pk_col] = range(1, n_rows + 1)\n",
    "            dim_pks[table_name] = synth_df[pk_col].unique()\n",
    "        \n",
    "        # Add FK for fact table, use actual dim PK values for correlation\n",
    "        if table_name == metadata[\"fact_table\"]:\n",
    "            for rel in metadata[\"relationships\"]:\n",
    "                if rel[\"from_table\"] == table_name and rel[\"to_table\"] in dim_pks:\n",
    "                    pk_vals = dim_pks[rel[\"to_table\"]]\n",
    "                    synth_df[rel[\"from_column\"]] = np.random.choice(pk_vals, n_rows)\n",
    "                elif rel[\"from_column\"] not in synth_df.columns:\n",
    "                    # Fallback FK\n",
    "                    synth_df[rel[\"from_column\"]] = np.random.choice(range(1, n_rows + 1), n_rows)\n",
    "        \n",
    "        # Ensure columns exist for FKs in dims\n",
    "        for rel in metadata[\"relationships\"]:\n",
    "            if rel[\"to_table\"] == table_name and rel[\"to_column\"] not in synth_df.columns:\n",
    "                synth_df[rel[\"to_column\"]] = range(1, n_rows + 1)\n",
    "        \n",
    "        # Enhanced correlation: Simple example, if 'quantity' and 'total_price' exist, set total_price = quantity * unit_price\n",
    "        if 'quantity' in synth_df.columns and 'total_price' in synth_df.columns:\n",
    "            if 'unit_price' not in synth_df.columns:\n",
    "                synth_df['unit_price'] = np.random.uniform(1, 100, n_rows)\n",
    "            synth_df['total_price'] = synth_df['quantity'] * synth_df['unit_price']\n",
    "        \n",
    "        synth_df.to_csv(f\"synthetic_data/synthetic_{table_name}.csv\", index=False)\n",
    "        synth_data[table_name] = synth_df\n",
    "        print(f\"Enhanced fallback synthetic {table_name}: {synth_df.shape}\")\n",
    "\n",
    "print(\"Synthetic data generated successfully with updated concerns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7f1aab-ca4f-482b-883f-8a4455d0c0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Validation with Actual Tables (Enhanced with Statistical Checks, Scores, and Correlation per Table)\n",
    "from scipy.stats import pearsonr, spearmanr, ks_2samp\n",
    "\n",
    "def validate_data(real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute validation metrics: distributions (KS test), correlations (Pearson/Spearman per table), FK consistency.\n",
    "    Updated: Added statistical scores (0-100) based on KS p-value, correlation diffs; per-table correlation validation.\n",
    "    \"\"\"\n",
    "    metrics = {\"distributions\": {}, \"correlations\": {}, \"fk_consistency\": {}, \"scores\": {}}\n",
    "    \n",
    "    overall_score = 0\n",
    "    table_scores = {}\n",
    "    \n",
    "    for table_name in real_data:\n",
    "        real_df = real_data[table_name]\n",
    "        synth_df = synth_data.get(table_name, pd.DataFrame())\n",
    "        if synth_df.empty:\n",
    "            continue\n",
    "        \n",
    "        table_score = 0\n",
    "        num_checks = 0\n",
    "        \n",
    "        # Distributions (KS for numerical)\n",
    "        for col in real_df.select_dtypes(include=[np.number]).columns:\n",
    "            if col in synth_df.columns:\n",
    "                real_vals = real_df[col].dropna()\n",
    "                synth_vals = synth_df[col].dropna()\n",
    "                if len(real_vals) > 0 and len(synth_vals) > 0:\n",
    "                    ks_stat, p_val = ks_2samp(real_vals, synth_vals)\n",
    "                    metrics[\"distributions\"][f\"{table_name}.{col}\"] = {\"ks_stat\": ks_stat, \"p_value\": p_val}\n",
    "                    # Score: higher p-value better (scale to 0-100)\n",
    "                    dist_score = min(100, p_val * 100)\n",
    "                    table_score += dist_score\n",
    "                    num_checks += 1\n",
    "        \n",
    "        # Correlations per table (Pearson for linear, Spearman for monotonic)\n",
    "        real_num = real_df.select_dtypes(include=[np.number]).dropna()\n",
    "        synth_num = synth_df.select_dtypes(include=[np.number]).dropna()\n",
    "        corr_metrics = {\"pearson\": {}, \"spearman\": {}}\n",
    "        for i, col1 in enumerate(real_num.columns):\n",
    "            for col2 in real_num.columns[i+1:]:\n",
    "                if col1 in synth_num.columns and col2 in synth_num.columns:\n",
    "                    # Pearson\n",
    "                    if len(real_num[col1]) > 1 and len(synth_num[col1]) > 1:\n",
    "                        corr_real_p, _ = pearsonr(real_num[col1], real_num[col2])\n",
    "                        corr_synth_p, _ = pearsonr(synth_num[col1], synth_num[col2])\n",
    "                        diff_p = abs(corr_real_p - corr_synth_p)\n",
    "                        corr_metrics[\"pearson\"][f\"{col1}_{col2}\"] = {\"real\": corr_real_p, \"synth\": corr_synth_p, \"diff\": diff_p}\n",
    "                        # Score: lower diff better (100 - diff*100, clipped)\n",
    "                        corr_score_p = max(0, 100 - diff_p * 100)\n",
    "                        table_score += corr_score_p\n",
    "                        num_checks += 1\n",
    "                    \n",
    "                    # Spearman\n",
    "                    corr_real_s, _ = spearmanr(real_num[col1], real_num[col2])\n",
    "                    corr_synth_s, _ = spearmanr(synth_num[col1], synth_num[col2])\n",
    "                    diff_s = abs(corr_real_s - corr_synth_s)\n",
    "                    corr_metrics[\"spearman\"][f\"{col1}_{col2}\"] = {\"real\": corr_real_s, \"synth\": corr_synth_s, \"diff\": diff_s}\n",
    "                    corr_score_s = max(0, 100 - diff_s * 100)\n",
    "                    table_score += corr_score_s\n",
    "                    num_checks += 1\n",
    "        \n",
    "        metrics[\"correlations\"][table_name] = corr_metrics\n",
    "        \n",
    "        # FK Consistency\n",
    "        for rel in [r for r in metadata[\"relationships\"] if r[\"from_table\"] == table_name or r[\"to_table\"] == table_name]:\n",
    "            if rel[\"from_table\"] == table_name and rel[\"from_column\"] in synth_df.columns:\n",
    "                fk_vals = synth_df[rel[\"from_column\"]].dropna().unique()\n",
    "                dim_df = synth_data.get(rel[\"to_table\"], pd.DataFrame())\n",
    "                if not dim_df.empty and rel[\"to_column\"] in dim_df.columns:\n",
    "                    pk_vals = set(dim_df[rel[\"to_column\"]].dropna().unique())\n",
    "                    coverage = len(set(fk_vals) & pk_vals) / len(set(fk_vals)) if len(set(fk_vals)) > 0 else 0\n",
    "                    metrics[\"fk_consistency\"][f\"{rel['from_table']}.{rel['from_column']} -> {rel['to_table']}.{rel['to_column']}\"] = {\"coverage\": coverage}\n",
    "                    # Score: coverage * 100\n",
    "                    fk_score = coverage * 100\n",
    "                    table_score += fk_score\n",
    "                    num_checks += 1\n",
    "        \n",
    "        # Table score average\n",
    "        if num_checks > 0:\n",
    "            table_scores[table_name] = table_score / num_checks\n",
    "            overall_score += table_scores[table_name]\n",
    "    \n",
    "    # Overall score\n",
    "    if len(table_scores) > 0:\n",
    "        metrics[\"scores\"][\"overall\"] = overall_score / len(table_scores)\n",
    "    else:\n",
    "        metrics[\"scores\"][\"overall\"] = 0\n",
    "    metrics[\"scores\"][\"per_table\"] = table_scores\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(\"validation_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(\"Validation complete. Metrics saved to validation_metrics.json\")\n",
    "    print(\"Overall Score (0-100):\", metrics[\"scores\"][\"overall\"])\n",
    "    print(\"Per-Table Scores:\", metrics[\"scores\"][\"per_table\"])\n",
    "    return metrics\n",
    "\n",
    "validation_metrics = validate_data(real_data, synth_data, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c991f6f-ce02-4250-a9d0-6576077be71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Dashboard Visualization (Using Specified Libraries, Inline Output)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline\n",
    "\n",
    "def create_dashboard(real_data: Dict[str, pd.DataFrame], synth_data: Dict[str, pd.DataFrame], metrics: Dict[str, Any], fact_table: str):\n",
    "    \"\"\"\n",
    "    Create inline dashboard using matplotlib, seaborn, and plotly to visualize real vs synthetic data.\n",
    "    Visualizations: histograms, box plots, correlation heatmaps, scatter plots for key correlations, FK coverage.\n",
    "    Displays all plots directly below the cell in Jupyter notebook.\n",
    "    \"\"\"\n",
    "    # Initialize figure counter for unique plot IDs\n",
    "    fig_counter = 1\n",
    "\n",
    "    # Dropdown simulation: Loop through each table to generate plots\n",
    "    for selected_table in real_data.keys():\n",
    "        print(f\"\\n--- Dashboard for Table: {selected_table} ---\")\n",
    "        \n",
    "        real_df = real_data[selected_table]\n",
    "        synth_df = synth_data.get(selected_table, pd.DataFrame())\n",
    "        \n",
    "        # Histograms for numerical columns (using matplotlib/seaborn)\n",
    "        num_cols = real_df.select_dtypes(include=[np.number]).columns[:3]  # Limit to 3 for clarity\n",
    "        if num_cols.size > 0:\n",
    "            plt.figure(figsize=(15, 5), num=f\"Figure {fig_counter}: Histograms - {selected_table}\")\n",
    "            for i, col in enumerate(num_cols, 1):\n",
    "                plt.subplot(1, len(num_cols), i)\n",
    "                sns.histplot(real_df[col], color='blue', alpha=0.5, label='Real', kde=True)\n",
    "                if col in synth_df.columns:\n",
    "                    sns.histplot(synth_df[col], color='orange', alpha=0.5, label='Synthetic', kde=True)\n",
    "                plt.title(f\"{col} Distribution\")\n",
    "                plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            fig_counter += 1\n",
    "        \n",
    "        # Box plots for numerical columns (using matplotlib/seaborn)\n",
    "        if num_cols.size > 0:\n",
    "            plt.figure(figsize=(15, 5), num=f\"Figure {fig_counter}: Box Plots - {selected_table}\")\n",
    "            for i, col in enumerate(num_cols, 1):\n",
    "                plt.subplot(1, len(num_cols), i)\n",
    "                data_to_plot = [real_df[col].dropna()]\n",
    "                labels = ['Real']\n",
    "                if col in synth_df.columns:\n",
    "                    data_to_plot.append(synth_df[col].dropna())\n",
    "                    labels.append('Synthetic')\n",
    "                sns.boxplot(data=data_to_plot, palette=['blue', 'orange'])\n",
    "                plt.xticks(range(len(labels)), labels)\n",
    "                plt.title(f\"{col} Box Plot\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            fig_counter += 1\n",
    "        \n",
    "        # Correlation heatmaps (using seaborn for real and synthetic)\n",
    "        real_num = real_df.select_dtypes(include=[np.number]).dropna()\n",
    "        synth_num = synth_df.select_dtypes(include=[np.number]).dropna()\n",
    "        if len(real_num.columns) > 1:\n",
    "            plt.figure(figsize=(12, 5), num=f\"Figure {fig_counter}: Correlation Heatmaps - {selected_table}\")\n",
    "            plt.subplot(1, 2, 1)\n",
    "            real_corr = real_num.corr(method='pearson')\n",
    "            sns.heatmap(real_corr, annot=True, cmap='RdBu', vmin=-1, vmax=1, center=0)\n",
    "            plt.title(\"Real Data Correlations\")\n",
    "            \n",
    "            if not synth_num.empty and len(synth_num.columns) > 1:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                synth_corr = synth_num.corr(method='pearson')\n",
    "                sns.heatmap(synth_corr, annot=True, cmap='RdBu', vmin=-1, vmax=1, center=0)\n",
    "                plt.title(\"Synthetic Data Correlations\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            fig_counter += 1\n",
    "        \n",
    "        # Scatter plots for key correlations (using plotly for interactivity)\n",
    "        if len(real_num.columns) >= 2:\n",
    "            corr_flat = real_corr.abs().unstack()\n",
    "            corr_flat = corr_flat[corr_flat < 1].sort_values(ascending=False)\n",
    "            if not corr_flat.empty:\n",
    "                col1, col2 = corr_flat.index[0]\n",
    "                fig_scatter = make_subplots(\n",
    "                    rows=1, cols=2, \n",
    "                    subplot_titles=[f\"Real: {col1} vs {col2}\", f\"Synthetic: {col1} vs {col2}\"],\n",
    "                    figure=go.Figure(layout=dict(height=400))\n",
    "                )\n",
    "                fig_scatter.add_trace(\n",
    "                    go.Scatter(x=real_df[col1], y=real_df[col2], mode='markers', name=\"Real\", marker=dict(color='blue')),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "                if col1 in synth_df.columns and col2 in synth_df.columns:\n",
    "                    fig_scatter.add_trace(\n",
    "                        go.Scatter(x=synth_df[col1], y=synth_df[col2], mode='markers', name=\"Synthetic\", marker=dict(color='orange')),\n",
    "                        row=1, col=2\n",
    "                    )\n",
    "                fig_scatter.update_layout(title=f\"Scatter Plots for {col1} vs {col2}\", showlegend=True)\n",
    "                fig_scatter.show()\n",
    "                fig_counter += 1\n",
    "            else:\n",
    "                print(f\"No significant correlations for scatter plot in {selected_table}\")\n",
    "        \n",
    "        # FK coverage (using plotly.express for bar plot)\n",
    "        fk_data = {k: v[\"coverage\"] for k, v in metrics[\"fk_consistency\"].items() if selected_table in k}\n",
    "        if fk_data:\n",
    "            fig_fk = px.bar(\n",
    "                x=list(fk_data.keys()), \n",
    "                y=list(fk_data.values()), \n",
    "                title=f\"FK Coverage (0-1) - {selected_table}\",\n",
    "                labels={'x': 'Relationship', 'y': 'Coverage'},\n",
    "                color_discrete_sequence=['blue'],\n",
    "                height=400\n",
    "            )\n",
    "            fig_fk.show()\n",
    "            fig_counter += 1\n",
    "        \n",
    "        # Metrics summary (text output)\n",
    "        table_metrics = {\n",
    "            \"distributions\": {k: v for k, v in metrics[\"distributions\"].items() if selected_table in k},\n",
    "            \"correlations\": metrics[\"correlations\"].get(selected_table, {}),\n",
    "            \"fk_consistency\": {k: v for k, v in metrics[\"fk_consistency\"].items() if selected_table in k},\n",
    "            \"score\": metrics[\"scores\"][\"per_table\"].get(selected_table, 0)\n",
    "        }\n",
    "        print(f\"\\nMetrics for {selected_table}:\")\n",
    "        print(json.dumps(table_metrics, indent=2))\n",
    "\n",
    "    print(\"\\nDashboard visualizations rendered inline for all tables.\")\n",
    "\n",
    "# Run the dashboard\n",
    "create_dashboard(real_data, synth_data, validation_metrics, metadata[\"fact_table\"])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
